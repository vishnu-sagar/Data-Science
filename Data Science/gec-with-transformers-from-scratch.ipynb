{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":34185.384493,"end_time":"2022-09-17T19:53:10.691337","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-09-17T10:23:25.306844","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk.translate.chrf_score\n# Import libraries\nimport torch\nimport pathlib as pl\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#plt.switch_backend('agg')\n\n# del transformer\n# torch.cuda.empty_cache()","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:32.747348Z","iopub.status.busy":"2022-09-17T10:23:32.746654Z","iopub.status.idle":"2022-09-17T10:23:35.846043Z","shell.execute_reply":"2022-09-17T10:23:35.845193Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":3.111993,"end_time":"2022-09-17T10:23:35.848322","exception":false,"start_time":"2022-09-17T10:23:32.736329","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Grammar Error Correction with Transformers\n======================================================\n\nThis notebook shows how to train a GEC model based on transformers and experiments with Transformer architecture in order to better understand how changes in the model architecture and in the training reflect in different performances.\n\n## Table of Contents\n\n- [1. Data Sourcing and Processing](#1)\n    - Tokenizing and Embedding\n    - Collation\n- [2. Seq2Seq Network using Transformer](#2)\n    - Positional encoding\n    - Multi-head attention\n- [3. Model definition](#3)\n- [4. Training](#4)\n- [5. Evaluation](#5)\n    - Greedy Search Inference\n    - Performance evaluation\n- [6. Experiments](#6)\n- [7. Results](#7)\n- [8. References](#8)","metadata":{"papermill":{"duration":0.007249,"end_time":"2022-09-17T10:23:35.864234","exception":false,"start_time":"2022-09-17T10:23:35.856985","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n# 1. Data Sourcing and Processing\n\nC4 200M dataset from Google Research is used in this notebook. You can find more information about the C4 200M dataset on Google Research's [BEA 2021 paper](https://aclanthology.org/2021.bea-1.4/) (Stahlberg and Kumar, 2021).\n\nThe already [processed dataset](https://huggingface.co/datasets/liweili/c4_200m) was extracted from Huggingface in CSV format, then was transformed to HDF5 format for better manageability. The conversion process is detailed in ``utils.py``, and was based on this [notebook](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb).\nThe final version of the dataset is uploaded on [Kaggle](https://www.kaggle.com/datasets/dariocioni/c4200m).\n\nA custom class ``Hdf5Dataset`` based on ``torch.utils.data.Dataset`` is developed, which yields a pair of source-target raw sentences.\n\n| source                                             | target                                                  |\n|----------------------------------------------------|---------------------------------------------------------|\n| Much many brands and sellers still in the market.  | Many brands and sellers still in the market.            |\n| She likes playing in park and come here every week | She likes playing in the park and comes here every week |\n\nTo be able to train on a arbitrary subset of a single file, the dataset only reads chunks of length ``num_entries`` if the parameter is specified . In order to capture more examples, a different chunk can be randomly chosen at each epoch by specifying ``randomized=True``","metadata":{"papermill":{"duration":0.00709,"end_time":"2022-09-17T10:23:35.878553","exception":false,"start_time":"2022-09-17T10:23:35.871463","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"import h5py\nfrom torch.utils.data import Dataset\nrandom.seed(42)\n\nclass Hdf5Dataset(Dataset):\n    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n\n    def __init__(self, h5_path, transform=None,num_entries = None,randomized=False):\n\n        self.h5f = h5py.File(h5_path, 'r')\n        self.size = self.h5f['labels'].shape[0]\n        self.transform = transform\n        self.randomized = randomized\n        self.max_index = num_entries if num_entries is not None else self.size\n        #Chooses an offset for the dataset when using a subset of a Hdf5 file\n        if randomized:\n            self.offset = random.choice(range(0,self.size//self.max_index))*self.max_index\n        else:\n            self.offset = 0\n\n\n    def __getitem__(self, index):\n        if index > self.max_index:\n            raise StopIteration\n        input = self.h5f['input'][self.offset+index].decode('utf-8')\n        label = self.h5f['labels'][self.offset+index].decode('utf-8')\n        if self.transform is not None:\n            features = self.transform(input)\n        return input, label\n\n    def __len__(self):\n        return self.max_index\n\n    def reshuffle(self):\n        if self.randomized:\n            self.offset = random.choice(range(0,self.size//self.max_index))*self.max_index\n        else:\n            print(\"Please set randomized=True\")","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:35.894868Z","iopub.status.busy":"2022-09-17T10:23:35.894051Z","iopub.status.idle":"2022-09-17T10:23:36.030206Z","shell.execute_reply":"2022-09-17T10:23:36.029462Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.146249,"end_time":"2022-09-17T10:23:36.032088","exception":false,"start_time":"2022-09-17T10:23:35.885839","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from typing import List\nfrom tqdm import tqdm\nfrom torchtext.data import get_tokenizer\n\nSRC_LANGUAGE = 'incorrect'\nTGT_LANGUAGE = 'correct'\nMAX_LENGTH = 500\nVOCAB_SIZE = 20000\n\nTRAIN_SAMPLES = 100000\nVALID_SAMPLES = 10000\n\n# Place-holders\nvocab_transform = {}\n\ntoken_transform = get_tokenizer(\"basic_english\")\n\n#folder = 'dataset'\n#train_filename = 'train.hf5'\n#valid_filename = 'valid.hf5'\n#test_filename = 'test.hf5'\n#src_vocab_path = 'vocab/src_vocab_20K_spacy.pth'\n#tgt_vocab_path = 'vocab/src_vocab_20K_spacy.pth'\n#embedding_path = 'vocab/glove_42B_300d_20K.pth'\n#checkpoint_folder = 'models'\n\n##COLAB\nfolder = '../input/c4200m/hdf5'\ntrain_filename = 'C4_200M.hf5-00000-of-00010'\nvalid_filename = 'C4_200M.hf5-00001-of-00010'\ntest_filename = 'C4_200M.hf5-00002-of-00010'\n# embedding_path = '/content/drive/MyDrive/Colab Notebooks/GEC_Soft_Masked_BERT/vocab/glove_42B_300d_20K.pth'\nsrc_vocab_path = '../input/c4-200m-vocabulary/vocab_20K.pth'\ntgt_vocab_path = '../input/c4-200m-vocabulary/vocab_20K.pth'\ncheckpoint_folder = './'","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:36.048587Z","iopub.status.busy":"2022-09-17T10:23:36.048312Z","iopub.status.idle":"2022-09-17T10:23:36.123906Z","shell.execute_reply":"2022-09-17T10:23:36.123178Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.085826,"end_time":"2022-09-17T10:23:36.125621","exception":false,"start_time":"2022-09-17T10:23:36.039795","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"papermill":{"duration":0.007191,"end_time":"2022-09-17T10:23:36.140726","exception":false,"start_time":"2022-09-17T10:23:36.133535","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"import torchtext as text\nimport numpy as np\nimport torch\n\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# # Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<UNK>','<PAD>', '<BOS>', '<EOS>']\n\nvocab_transform[SRC_LANGUAGE] = torch.load(src_vocab_path)\nvocab_transform[TGT_LANGUAGE] = torch.load(tgt_vocab_path)\n\n#Procedure for pretrained embeddings. Not fitting into memory\n# def pretrained_embs(name: str, dim: str,max_vectors: int=None):\n#     glove_vectors = text.vocab.GloVe(name=name,dim=dim,max_vectors=max_vectors)\n#     glove_vocab = text.vocab.vocab(glove_vectors.stoi)\n#     pretrained_embeddings = glove_vectors.vectors\n#     glove_vocab.insert_token('<UNK>',UNK_IDX)\n#     pretrained_embeddings = torch.cat((torch.mean(pretrained_embeddings,dim=0,keepdims=True),pretrained_embeddings))\n#     glove_vocab.insert_token('<PAD>',PAD_IDX)\n#     pretrained_embeddings = torch.cat((torch.zeros(1,pretrained_embeddings.shape[1]),pretrained_embeddings))\n#     glove_vocab.insert_token('<BOS>',PAD_IDX)\n#     pretrained_embeddings = torch.cat((torch.rand(1,pretrained_embeddings.shape[1]),pretrained_embeddings))\n#     glove_vocab.insert_token('<EOS>',PAD_IDX)\n#     pretrained_embeddings = torch.cat((torch.rand(1,pretrained_embeddings.shape[1]),pretrained_embeddings))\n#     glove_vocab.set_default_index(UNK_IDX)\n#     return glove_vocab,pretrained_embeddings\n# vocab, embeddings = pretrained_embs('42B','100',50000)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:36.157577Z","iopub.status.busy":"2022-09-17T10:23:36.156890Z","iopub.status.idle":"2022-09-17T10:23:36.355669Z","shell.execute_reply":"2022-09-17T10:23:36.354808Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.209788,"end_time":"2022-09-17T10:23:36.357832","exception":false,"start_time":"2022-09-17T10:23:36.148044","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Collation\n\nAn iterator over ``Hdf5dataset`` yields a pair of raw strings.\nWe need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network.\nBelow I defined a collate function that converts batch of raw strings into batch tensors that can be fed directly into the model.","metadata":{"papermill":{"duration":0.007479,"end_time":"2022-09-17T10:23:36.373042","exception":false,"start_time":"2022-09-17T10:23:36.365563","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\n# helper function to club together sequential operations\ndef sequential_transforms(*transforms):\n    def func(txt_input):\n        for transform in transforms:\n            txt_input = transform(txt_input)\n        return txt_input\n    return func\n\n# def glove_transform(tokens: List[str]):\n\n# function to add BOS/EOS and create tensor for input sequence indices\ndef tensor_transform(token_ids: List[int]):\n    #truncate sequences longer than MAX_LENGTH\n    if(len(token_ids) > MAX_LENGTH - 2):\n        token_ids = token_ids[0:MAX_LENGTH -2]\n    return torch.cat((torch.tensor([BOS_IDX]),\n                      torch.tensor(token_ids),\n                      torch.tensor([EOS_IDX])))\n\n# src and tgt language text transforms to convert raw strings into tensors indices\ntext_transform = {}\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    text_transform[ln] = sequential_transforms(token_transform,\n                                           vocab_transform[ln],\n                                           tensor_transform) # Add BOS/EOS and create tensor\n\n\n# function to collate data samples into batch tesors\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n    return src_batch, tgt_batch","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:36.389850Z","iopub.status.busy":"2022-09-17T10:23:36.389145Z","iopub.status.idle":"2022-09-17T10:23:36.509688Z","shell.execute_reply":"2022-09-17T10:23:36.508914Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.130625,"end_time":"2022-09-17T10:23:36.511423","exception":false,"start_time":"2022-09-17T10:23:36.380798","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Here's an example of the encoding of the source and target sentences","metadata":{"papermill":{"duration":0.007272,"end_time":"2022-09-17T10:23:36.526264","exception":false,"start_time":"2022-09-17T10:23:36.518992","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"text = ('Data Maining is awesome!','Data Mining is awesome!')\nsrc,tgt = collate_fn([text])\nprint(src,tgt)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:36.542415Z","iopub.status.busy":"2022-09-17T10:23:36.541752Z","iopub.status.idle":"2022-09-17T10:23:36.551259Z","shell.execute_reply":"2022-09-17T10:23:36.550504Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.019745,"end_time":"2022-09-17T10:23:36.553281","exception":false,"start_time":"2022-09-17T10:23:36.533536","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[   2],\n\n        [ 157],\n\n        [   0],\n\n        [  13],\n\n        [1480],\n\n        [  32],\n\n        [   3]]) tensor([[   2],\n\n        [ 157],\n\n        [1185],\n\n        [  13],\n\n        [1480],\n\n        [  32],\n\n        [   3]])\n"}]},{"cell_type":"markdown","source":"<a name='2'></a>\n# 2. Seq2Seq Network using Transformer\n\nTransformer is a Seq2Seq model introduced in [“Attention is all you need”](<https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>) paper for solving machine translation tasks.\nBelow, we will create a Seq2Seq network that uses Transformer. The network consists of three parts:\n1) The embedding layer. This layer converts tensor of input indices into corresponding tensor of input embeddings.\n    These embedding are further augmented with ``Positional Encodings``, to provide position information of input tokens to the model.\n2) The actual [Transformer](<https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>) model.\n3) The output of Transformer model is finally passed through a linear layer that give un-normalized probabilities for each token in the target language.\n\n### Positional Encoding\nDifferently from RNNs, Transformers don't have a notion of relative or absolute position of the tokens in the input.\nOne solution is to combine the input embeddings with positional embeddings, specific to each position in an input sequence.\nA solution that is not biased towards the initial positions consists in a combination of sine and cosine functions of different frequencies ([Vaswani et al. ,2017](<https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>))\n\nGiven an embedding of length $d$, a position in the sequence $pos$ and the $i$-th dimension of the embedding, the position embedding is calculated as\n\n$$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d})\\quad,\\quad PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d})$$\n\nDropout is also added to the sums of the embeddings and the positional encodings in both the encoder and decoder.\n\n<img src=\"https://github.com/ciodar/GEC_Methods/blob/master/img/pos_enc.png?raw=true\">\n\n### Multi-head attention\nA single transformer block cannot capture all the different kinds of simultaneous relations among its inputs.\nTo address this problem, Transformers can use multiple self-attention heads, residing in parallel layers and with different parameter sets.\nEach head $i$ will have a different set of key, query and value matrices $W_i^K,W_i^Q,W_i^V$ and will project into different embeddings for each head.\nThe different embeddings are finally reduced to the original input dimension, using a trainable linear projection $W^O$\n\n<img src=\"https://github.com/ciodar/GEC_Methods/blob/master/img/multihead.png?raw=true\">\n\n\n","metadata":{"papermill":{"duration":0.007266,"end_time":"2022-09-17T10:23:36.568504","exception":false,"start_time":"2022-09-17T10:23:36.561238","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"from torch import Tensor\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Transformer\n\nimport math\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size,embedding_weights=None):\n        super(TokenEmbedding, self).__init__()\n        if embedding_weights is not None:\n            self.embedding = nn.Embedding.from_pretrained(embedding_weights,freeze=True,padding_idx=PAD_IDX)\n        else:\n            self.embedding = nn.Embedding(vocab_size, emb_size)\n\n            # self.embedding.weight.requires_grad =False\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network \nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 100,\n                 dropout: float = 0.0,\n                 embedding_weights = None):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size,embedding_weights)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size,embedding_weights)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor, src_padding_mask: Tensor = None):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask, src_padding_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:36.584956Z","iopub.status.busy":"2022-09-17T10:23:36.584688Z","iopub.status.idle":"2022-09-17T10:23:36.659245Z","shell.execute_reply":"2022-09-17T10:23:36.658439Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.084901,"end_time":"2022-09-17T10:23:36.660834","exception":false,"start_time":"2022-09-17T10:23:36.575933","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Two different sets of masks are needed during training:\n- A subsequent word mask that will prevent model to look into the future words when making predictions.\n- A mask to hide source and target padding tokens and prevent to compute attention on them.","metadata":{"papermill":{"duration":0.007227,"end_time":"2022-09-17T10:23:36.675711","exception":false,"start_time":"2022-09-17T10:23:36.668484","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:36.692205Z","iopub.status.busy":"2022-09-17T10:23:36.691501Z","iopub.status.idle":"2022-09-17T10:23:36.698274Z","shell.execute_reply":"2022-09-17T10:23:36.697592Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.016907,"end_time":"2022-09-17T10:23:36.699935","exception":false,"start_time":"2022-09-17T10:23:36.683028","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"example_mask = create_mask(src,tgt)\n_ = plt.imshow(example_mask[1].cpu(),cmap='hot')\nplt.show()","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:36.716077Z","iopub.status.busy":"2022-09-17T10:23:36.715458Z","iopub.status.idle":"2022-09-17T10:23:39.929635Z","shell.execute_reply":"2022-09-17T10:23:39.928838Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":3.224235,"end_time":"2022-09-17T10:23:39.931522","exception":false,"start_time":"2022-09-17T10:23:36.707287","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"<a name=\"3\"></a>\n# 3. Model definition\nThe network is instantiated as a  ``Seq2SeqTransformer`` object, where the following hyperparameters can be changed:\n- Embedding Size\n- Number of Transformer layers in the Encoder and Decoder\n- Units in the Feed Forward Network\n- Number of attention heads\n- The dropout between each sublayer of the Transformer and in the sum of the embeddings with the positional encoding\n","metadata":{"papermill":{"duration":0.007699,"end_time":"2022-09-17T10:23:39.947047","exception":false,"start_time":"2022-09-17T10:23:39.939348","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"torch.manual_seed(0)\n\nVOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE].vocab.itos_)\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 1024\nBATCH_SIZE = 8\nNUM_ENCODER_LAYERS = 6\nNUM_DECODER_LAYERS = 6\n\n#To save and retrieve checkpoints\nmodel_name = 'transformer_%dE_%dH_%dF_%d_%d.pt'%(EMB_SIZE,NHEAD,FFN_HID_DIM,NUM_DECODER_LAYERS,NUM_DECODER_LAYERS)\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n                                 NHEAD, VOCAB_SIZE, VOCAB_SIZE, FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\ntransformer = transformer.to(DEVICE)\n\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX,label_smoothing=0.1)\n\noptimizer = torch.optim.Adam(transformer.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-9)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:39.963429Z","iopub.status.busy":"2022-09-17T10:23:39.963119Z","iopub.status.idle":"2022-09-17T10:23:41.158217Z","shell.execute_reply":"2022-09-17T10:23:41.157360Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":1.205852,"end_time":"2022-09-17T10:23:41.160514","exception":false,"start_time":"2022-09-17T10:23:39.954662","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import IterableDataset\n\ndef train_epoch(model, optimizer):\n    model.train()\n    losses = 0\n    train_iter = Hdf5Dataset(pl.Path(folder)/train_filename,num_entries=TRAIN_SAMPLES,randomized=True)\n    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\n    for src, tgt in train_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        optimizer.zero_grad()\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n\n        optimizer.step()\n        losses += loss.item()\n\n    return losses / len(train_dataloader)\n\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n\n    val_iter = Hdf5Dataset(pl.Path(folder)/valid_filename,num_entries=VALID_SAMPLES)\n    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\n    for src, tgt in val_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n        with torch.no_grad():\n            logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n        \n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n    return losses / len(val_dataloader)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:41.178470Z","iopub.status.busy":"2022-09-17T10:23:41.176996Z","iopub.status.idle":"2022-09-17T10:23:41.188290Z","shell.execute_reply":"2022-09-17T10:23:41.187571Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.021558,"end_time":"2022-09-17T10:23:41.189942","exception":false,"start_time":"2022-09-17T10:23:41.168384","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"4\"></a>\n# 4. Training\nThe training is performed on 20 epochs, and a checkpoint is saved at each epoch. The entire training process requires approximately 3 hours to complete on Kaggle.\n\nI used the cross-entropy loss as the loss function and Adam optimizer for training.\nThe regularization hyperparameters were chosen as in ([Vaswani et al. ,2017](<https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>))\n\n- learning rate = $10^{-4}$\n- $\\beta_1 = 0.9$\n- $\\beta_2 = 0.98$\n- $\\hat{\\varepsilon} = 10^{-9}$\n\n## Label Smoothing\nLabel smoothing is a regularization technique that introduces noise in the labels.\nThis way, we can avoid to obtain a model that is over-confident on certain tokens (ex. the UNK token), and give a lesser probability also on other tokens.","metadata":{"papermill":{"duration":0.007782,"end_time":"2022-09-17T10:23:41.205376","exception":false,"start_time":"2022-09-17T10:23:41.197594","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"#checkpoint = torch.load(\"../input/transformer-1h-32b-nopt/transformer_glove100E_2H_128F_3_3.pt\")\n#transformer.load_state_dict(checkpoint['model_state_dict'])\n#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#epoch = checkpoint['epoch']","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:41.222031Z","iopub.status.busy":"2022-09-17T10:23:41.221489Z","iopub.status.idle":"2022-09-17T10:23:41.224840Z","shell.execute_reply":"2022-09-17T10:23:41.224168Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.013466,"end_time":"2022-09-17T10:23:41.226446","exception":false,"start_time":"2022-09-17T10:23:41.212980","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from timeit import default_timer as timer\nNUM_EPOCHS = 40\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': transformer.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': val_loss,\n    }, pl.Path(checkpoint_folder)/model_name)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T10:23:41.242851Z","iopub.status.busy":"2022-09-17T10:23:41.242242Z","iopub.status.idle":"2022-09-17T19:36:39.664235Z","shell.execute_reply":"2022-09-17T19:36:39.662855Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":33178.43962,"end_time":"2022-09-17T19:36:39.673483","exception":false,"start_time":"2022-09-17T10:23:41.233863","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"Epoch: 1, Train loss: 5.235, Val loss: 3.967, Epoch time = 794.327s\n\nEpoch: 2, Train loss: 3.587, Val loss: 3.354, Epoch time = 790.010s\n\nEpoch: 3, Train loss: 3.262, Val loss: 3.176, Epoch time = 786.591s\n\nEpoch: 4, Train loss: 3.135, Val loss: 3.098, Epoch time = 790.520s\n\nEpoch: 5, Train loss: 3.072, Val loss: 3.042, Epoch time = 805.762s\n\nEpoch: 6, Train loss: 3.031, Val loss: 3.011, Epoch time = 809.789s\n\nEpoch: 7, Train loss: 3.002, Val loss: 2.984, Epoch time = 806.896s\n\nEpoch: 8, Train loss: 2.975, Val loss: 2.963, Epoch time = 806.751s\n\nEpoch: 9, Train loss: 2.954, Val loss: 2.943, Epoch time = 808.415s\n\nEpoch: 10, Train loss: 2.930, Val loss: 2.917, Epoch time = 809.460s\n\nEpoch: 11, Train loss: 2.901, Val loss: 2.888, Epoch time = 811.251s\n\nEpoch: 12, Train loss: 2.871, Val loss: 2.867, Epoch time = 807.540s\n\nEpoch: 13, Train loss: 2.837, Val loss: 2.813, Epoch time = 807.482s\n\nEpoch: 14, Train loss: 2.806, Val loss: 2.782, Epoch time = 807.033s\n\nEpoch: 15, Train loss: 2.779, Val loss: 2.769, Epoch time = 808.448s\n\nEpoch: 16, Train loss: 2.755, Val loss: 2.736, Epoch time = 808.059s\n\nEpoch: 17, Train loss: 2.728, Val loss: 2.716, Epoch time = 793.777s\n\nEpoch: 18, Train loss: 2.704, Val loss: 2.697, Epoch time = 787.960s\n\nEpoch: 19, Train loss: 2.677, Val loss: 2.677, Epoch time = 788.766s\n\nEpoch: 20, Train loss: 2.661, Val loss: 2.649, Epoch time = 789.477s\n\nEpoch: 21, Train loss: 2.647, Val loss: 2.634, Epoch time = 786.450s\n\nEpoch: 22, Train loss: 2.630, Val loss: 2.623, Epoch time = 788.150s\n\nEpoch: 23, Train loss: 2.602, Val loss: 2.601, Epoch time = 790.489s\n\nEpoch: 24, Train loss: 2.585, Val loss: 2.579, Epoch time = 793.121s\n\nEpoch: 25, Train loss: 2.572, Val loss: 2.574, Epoch time = 789.322s\n\nEpoch: 26, Train loss: 2.563, Val loss: 2.575, Epoch time = 789.789s\n\nEpoch: 27, Train loss: 2.569, Val loss: 2.577, Epoch time = 787.240s\n\nEpoch: 28, Train loss: 2.560, Val loss: 2.553, Epoch time = 789.333s\n\nEpoch: 29, Train loss: 2.552, Val loss: 2.557, Epoch time = 788.221s\n\nEpoch: 30, Train loss: 2.547, Val loss: 2.542, Epoch time = 785.965s\n\nEpoch: 31, Train loss: 2.536, Val loss: 2.525, Epoch time = 785.614s\n\nEpoch: 32, Train loss: 2.527, Val loss: 2.526, Epoch time = 787.149s\n\nEpoch: 33, Train loss: 2.521, Val loss: 2.518, Epoch time = 787.374s\n\nEpoch: 34, Train loss: 2.512, Val loss: 2.503, Epoch time = 785.210s\n\nEpoch: 35, Train loss: 2.495, Val loss: 2.493, Epoch time = 788.424s\n\nEpoch: 36, Train loss: 2.493, Val loss: 2.497, Epoch time = 790.129s\n\nEpoch: 37, Train loss: 2.474, Val loss: 2.492, Epoch time = 784.639s\n\nEpoch: 38, Train loss: 2.481, Val loss: 2.489, Epoch time = 785.614s\n\nEpoch: 39, Train loss: 2.471, Val loss: 2.476, Epoch time = 785.036s\n\nEpoch: 40, Train loss: 2.466, Val loss: 2.474, Epoch time = 785.699s\n"}]},{"cell_type":"code","source":"_ = plt.plot(train_losses)\n_ = plt.plot(val_losses)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T19:36:39.782267Z","iopub.status.busy":"2022-09-17T19:36:39.781892Z","iopub.status.idle":"2022-09-17T19:36:39.962799Z","shell.execute_reply":"2022-09-17T19:36:39.961997Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.193306,"end_time":"2022-09-17T19:36:39.964582","exception":false,"start_time":"2022-09-17T19:36:39.771276","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"<a name=\"5\"></a>\n# 5. Evaluation\n\n## Greedy Search inference\nWe can perform corrections of sentences using the ``greedy_decode`` function.\nIn this function, at each timestep the most probable token is selected as correction at each time step, without evaluating its impact on the rest of the sequence.\n\nI provide a batched implementation of greedy search for higher inference speed, based on a simplification of [HuggingFace Pytorch implementation](https://github.com/huggingface/transformers/blob/8f2723caf0f1bf7e1f639d28d004f81c96d19bbc/src/transformers/generation_utils.py) .\nIn this implementation, the model is forced to predict only PAD tokens after the character EOS is predicted, using the following formula:\n\n$$\\hat{y}_t = \\hat{y}_t * u_t + (1-u_t) * \\text{<PAD>}$$\n\nwhere $\\hat{y}_t$ is a vector containing the index of the highest probability word at time $t$ and $u_t$ is a boolean vector which is set to 1 if a EOS token is has not been predicted yet for the i-th sentence of the batch and 0 otherwise.\nThe prediction ends if $u_t[i] == 0 \\; \\forall  i$ or if the maximum sequence length is reached.","metadata":{"papermill":{"duration":0.009601,"end_time":"2022-09-17T19:36:39.984258","exception":false,"start_time":"2022-09-17T19:36:39.974657","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"# function to generate output sequence using greedy algorithm\n# input: (input_length,batch_size)\n# output: (input_length)\ndef greedy_decode(model, src, src_mask, src_padding_mask, max_len):\n    batch_size = src.shape[1]\n\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n    src_padding_mask = src_padding_mask.to(DEVICE)\n    unfinished_sequences = torch.ones(1,batch_size).to(DEVICE)\n\n    context = transformer.encode(src, src_mask,src_padding_mask).to(DEVICE)\n    ys = torch.ones(1, batch_size).fill_(BOS_IDX).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, context, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob,dim=1)\n\n        #Predict only PAD_IDX after EOS_IDX is predicted\n        next_word = next_word * unfinished_sequences + (1-unfinished_sequences) * PAD_IDX\n        ys = torch.cat([ys,\n                        next_word], dim=0)\n\n        # if eos_token was found in one sentence, set sentence to finished\n        unfinished_sequences = unfinished_sequences.mul((next_word != EOS_IDX).long())\n\n        # stop when each sentence is finished, or if we exceed the maximum length\n        if unfinished_sequences.max() == 0:\n            break\n    return ys.int()\n\n\n# actual function to correct input sentence\ndef correct(src_sentence: str, model: torch.nn.Module):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n    src_padding_mask = (torch.zeros(1, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask,src_padding_mask, max_len=num_tokens + 5).flatten()\n    return ' '.join([vocab_transform[SRC_LANGUAGE].vocab.itos_[i] for i in tgt_tokens if i not in [PAD_IDX,BOS_IDX,EOS_IDX]])","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T19:36:40.004928Z","iopub.status.busy":"2022-09-17T19:36:40.004650Z","iopub.status.idle":"2022-09-17T19:36:40.015886Z","shell.execute_reply":"2022-09-17T19:36:40.015170Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023503,"end_time":"2022-09-17T19:36:40.017550","exception":false,"start_time":"2022-09-17T19:36:39.994047","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"A better way could be to use a ``Beam Search``, choosing $k$ tokens at each timestep and then scoring each node with its log probability.","metadata":{"papermill":{"duration":0.009496,"end_time":"2022-09-17T19:36:40.036790","exception":false,"start_time":"2022-09-17T19:36:40.027294","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"checkpoint = torch.load(pl.Path(checkpoint_folder)/model_name)\ntransformer.load_state_dict(checkpoint['model_state_dict'])\n\ntransformer.eval()\n\n# Pick one in 18M examples\ntest_iter = Hdf5Dataset(pl.Path(folder)/test_filename,num_entries=None)\n\nsrc,trg = random.choice(test_iter)\n\nprint(\"input: \\\"\",src,\"\\\"\")\nprint(\"target: \\\"\",trg,\"\\\"\")\n\nprediction = correct(src,transformer)\n\nprint(\"prediction: \\\"%s\\\"\"%prediction)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T19:36:40.057285Z","iopub.status.busy":"2022-09-17T19:36:40.057000Z","iopub.status.idle":"2022-09-17T19:36:40.949524Z","shell.execute_reply":"2022-09-17T19:36:40.948727Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.905467,"end_time":"2022-09-17T19:36:40.952006","exception":false,"start_time":"2022-09-17T19:36:40.046539","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":16,"outputs":[{"name":"stdout","output_type":"stream","text":"input: \" Stemware, case bulk discounts and more for your event. \"\n\ntarget: \" Stemware, bulk case discounts and more for your event. \"\n\nprediction: \"<UNK> , case bulk discounts and more for your event .\"\n"}]},{"cell_type":"markdown","source":"## Performance evaluation\nPerformance is evaluated using the chrF score, which computes an F-score on character $n$-grams ([Popovi ́c, 2015](https://aclanthology.org/W15-3049/)).\nThe formula for the chrF score is\n\n$$\\text{chrF}_\\beta = (1+ \\beta^2) \\frac{\\text{chrP} \\cdot \\text{chrR}}{\\beta^2 \\cdot \\text{chrP} + \\text{chrR}}$$\n\nwhere chrP and chrR are $n$-gram precision and recall averaged over all $n$-grams.\n\nIn the experiments I used character $n$-grams between 1 and 6 and $\\beta=3$. White space between words is ignored during scoring.","metadata":{"papermill":{"duration":0.015481,"end_time":"2022-09-17T19:36:40.984137","exception":false,"start_time":"2022-09-17T19:36:40.968656","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom nltk.translate import chrf_score,bleu_score\nimport re\n\ndef test_collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(tgt_sample.rstrip(\"\\n\"))\n\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n    return src_batch, tgt_batch\n\ndef test(model):\n    model.eval()\n    score = 0\n\n    test_iter = Hdf5Dataset(pl.Path(folder)/test_filename,num_entries=VALID_SAMPLES)\n    test_dataloader = DataLoader(test_iter, batch_size=64,collate_fn=test_collate_fn)\n    for src, tgt in test_dataloader:\n        src_mask, _, src_padding_mask, _ = create_mask(src, src)\n        num_tokens = src.shape[0]\n        with torch.no_grad():\n            pred_tokens = greedy_decode(\n                model,  src, src_mask,src_padding_mask, max_len=num_tokens + 5).T.flatten()\n            pred_sentences = ' '.join([vocab_transform[SRC_LANGUAGE].vocab.itos_[i] for i in pred_tokens if i not in [PAD_IDX,BOS_IDX,EOS_IDX]])\n            pred_sentences = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', pred_sentences)\n        score += chrf_score.sentence_chrf(pred_sentences,' '.join(tgt).lower())\n\n    return score / len(test_dataloader)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T19:36:41.017537Z","iopub.status.busy":"2022-09-17T19:36:41.017083Z","iopub.status.idle":"2022-09-17T19:36:41.030908Z","shell.execute_reply":"2022-09-17T19:36:41.030193Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.033117,"end_time":"2022-09-17T19:36:41.033295","exception":false,"start_time":"2022-09-17T19:36:41.000178","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(pl.Path(checkpoint_folder)/model_name)\ntransformer.load_state_dict(checkpoint['model_state_dict'])\n\ntransformer.eval()\n\n# print(chrf_score.chrf_precision_recall_fscore_support('Thinking of hiring The Pass Street Food Cafe for your event?','thinking of hiring the pass street food cafe for your event',n=1))\n\ntest_loss = test(transformer)\nprint(test_loss)\n\n# bleu = bleu_score.sentence_bleu()\n#\n# test_loss = test(transformer,bleu)\n# print(test_loss)","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2022-09-17T19:36:41.067674Z","iopub.status.busy":"2022-09-17T19:36:41.067290Z","iopub.status.idle":"2022-09-17T19:53:09.327573Z","shell.execute_reply":"2022-09-17T19:53:09.326739Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":988.289571,"end_time":"2022-09-17T19:53:09.339658","exception":false,"start_time":"2022-09-17T19:36:41.050087","status":"completed"},"pycharm":{"name":"#%%\n"},"tags":[]},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"0.6867695216285437\n"}]},{"cell_type":"markdown","source":"<a name=\"6\"></a>\n# 6. Experiments\n\n## Attention heads\nI have evaluated how the number of attention heads is related with performance by training models with a different number of attention heads, and by varying also the embedding dimension.\nIn rows (A), we can see that an increase of the number of attention heads alone does not bring an increase in performance: moreover, the model with 2 attention heads performs significantly worse than the model with only one attention heads.\nA motivation for this could be the fact that the increase of the number of attention heads reduces the dimensionality of each head, since the dimensionality is given by\n\n$$d_k=d_v=d_{emb}/h$$\n\nWhere $d_k$ is the dimension of the key vector, $d_v$ the dimension of the value vector of the attention, $d_{emb}$ is the length of the embedding vectors and h is the number of heads.\n\nIf a multi-head attention approach is combined with an increase of the embedding dimension, the models reach a better performance than the base model, and also better than the increase of the embedding dimension alone.\n\n## Feed Forward dimension\nA second experiment evaluated the performance of models with a different dimension of the feed forward network of each Transformer layer.\nThe experiments in rows (B) show the dimension of the base model of 256 is the best performing dimension, and bigger networks did not perform better.\n\n## Transformer layers\nA third experiment evaluated the performance of models with a different number of Transformer layers.\nIn rows (C), the model with only 2 Transformer layers reached the same performance of the base model on the test set, while bigger models show worse performance.\n\n## Dropout\nThe last experiment evaluated the performance of models by changing the dropout that is applied to the output of each sub-layer of the Transformer unit, and also to the sum of the embeddings and the positional encodings in both the encoder and the decoder.\nIn Table 2 rows (D), the model without dropout shows lower perplexity, but shows worse performance on the chrF metric calculated on the test dataset, which confirms the importance of the  dropout to enable a better generalization of the model. Higher dropout values did not bring an improvement to the current training process.\n\n## Label smoothing\nThe model with dropout and label smoothing achieved a chrF score slightly worse than the base model (which did not use label smoothing) , and a much higher perplexity score (as expected) .\n\n|         | $N$ | $d_{emb}$ | $d_{\\text{ff}}$ | $h$ | $P_{\\text{dropout}}$ | $\\varepsilon_{ls}$ | train epochs | PPL(valid) | chrF(test) | parameters $\\times 10^6$ |\n|---------|-----|-----------|-----------------|:---:|----------------------|--------------------|--------------|------------|------------|--------------------------|\n| base    | 3   | 100       | 256             | 1   | 0.1                  |                    | 20           | 5.91       | 0.74       | 6.7                      |\n| (A)     |     |           |                 | 2   |                      |                    |              | 6.03       | 0.58       |                          |\n| (A)     |     | 200       |                 | 2   |                      |                    |              | 4.58       | 0.76       | 14.1                     |\n| (A)     |     | 200       |                 |     |                      |                    |              | 4.63       | 0.75       | 14.1                     |\n| (A)     |     |           |                 | 4   |                      |                    |              | 5.93       | 0.74       |                          |\n| (A)     |     | 400       |                 | 4   |                      |                    |              | 3.80       | 0.77       | 31.0                     |\n| (A)     |     | 400       |                 |     |                      |                    |              | 3.87       | 0.74       | 31.0                     |\n| (A)     |     |           |                 | 5   |                      |                    |              | 6.02       | 0.73       | 6.7                      |\n| (B)     |     |           | 128             |     |                      |                    |              | 5.85       | 0.66       | 6.5                      |\n| (B)     |     |           | 512             |     |                      |                    |              | 7.77       | 0.52       | 7.0                      |\n| (C)     | 1   |           |                 |     |                      |                    |              | 6.99       | 0.65       | 6.2                      |\n| (C)     | 2   |           |                 |     |                      |                    |              | **5.85**   | **0.74**   | 6.5                      |\n| (C)     | 4   |           |                 |     |                      |                    |              | 7.92       | 0.66       | 6.9                      |\n| (C)     | 5   |           |                 |     |                      |                    |              | 7.97       | 0.51       | 7.1                      |\n| (D)     |     |           |                 |     | 0                    |                    |              | 4.78       | 0.46       | 6.7                      |\n| (D)     |     |           |                 |     | 0.2                  |                    |              | 7.51       | 0.74       | 6.7                      |\n| (D)     |     |           |                 |     | 0                    |                    |              |            |            |                          |\n| (D)     |     |           |                 |     | 0.1                  |                    |              |            |            |                          |\n| large   | 6   | 256       | 512             | 4   |                      | 0.1                | 20           | 14.3       | 0.72       | 19.3                     |\n| X-large | 6   | 512       | 1024            | 8   |                      |                    | 40           |            | 0.09       | 62.0                     |\n\nOther experiments with changes of the vocabulary dimension or using pretrained vocabularies did not show acceptable results.\n\n|   | Embeddings                 | N | $d_{\\text{ff}}$ | $h$ | chrF(test) |\n|---|----------------------------|---|-----------------|-----|------------|\n|   | No pretraining (10K vocab) | 2 | 128             | 2   | 0.59       |\n|   | No pretraining (20K vocab) | 2 | 128             | 2   | 0.64       |\n|   | No pretraining (50K vocab) | 2 | 128             | 2   | 0.65       |\n|   | GloVe Twitter 100D         | 2 | 128             | 2   | 0.47       |\n|   | GloVe 6B 100D (50K vocab)  | 3 | 256             | 5   | 0.44       |","metadata":{"papermill":{"duration":0.009627,"end_time":"2022-09-17T19:53:09.359895","exception":false,"start_time":"2022-09-17T19:53:09.350268","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"markdown","source":"<a name=\"7\"></a>\n# Discussion\n\n## Training process\nTransformer models usually require a much longer training time in order to obtain satisfactory results. The previous experiments were constrained in time and in GPU memory capacity, but still managed to obtain a model with good representation of the english language.\nThe training method used in the experiments is also much simpler than the one used in ([Vaswani et al. ,2017](<https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>)), where it was used a learning rate schedule, with linearly increasing learning rate during the warmup phase and an inverse square root annealing.\n\nThe X-large model training could not converge with the used training process. A lower learning rate or a more sophisticated training process could have helped the convergence. In every other experiment the training process reached the convergence without any issues.\n\n## Rare word problem\nWord embeddings show major limitations when the model needs to correct out-of-vocabulary tokens ([Yuan and Briscoe, 2016](https://aclanthology.org/N16-1042/)).\n- The vocabulary size is limited due to memory constraints, therefore rare words, such as proper nouns or misspelled words are replaced with the UNK token.\n- Using the same vocabulary for the input and the target, the model learns to generate an OOV token when the inputs contains one and not tries to perform inference of a likely word in the context.\nThis model is not able to correct misspelled words, but also cannot keep some error-free original rare words.\n\nThe solution described in ([Yuan and Briscoe, 2016](https://aclanthology.org/N16-1042/)) requires the alignment of the tokens in the source and target sequences, and translate separately UNK tokens with a different model.\nAn alternative approach could be to use Wordpiece vocabularies, where unknown words are split into subwords, such as in BERT model.\n\n## Inference\n\nThe greedy search approach shows evident limits in the prediction of the first tokens, and may not find the highest probability sentence.\nA more powerful approach is the Beam Search, in which the model carries along several generated sequences, which can be represented as a tree where each branch corresponds to a different sequence. At the end, the sequence with highest probility is chosen.\nWhile this method brings much better results, it has a higher memory footprint and a much higher complexity in the algorithm, thus it was not used in the experiments.","metadata":{"papermill":{"duration":0.009718,"end_time":"2022-09-17T19:53:09.379561","exception":false,"start_time":"2022-09-17T19:53:09.369843","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}},{"cell_type":"markdown","source":"<a name=\"8\"></a>\n# References\n- [Attention is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n- [Pytorch Transformer tutorials](https://pytorch.org/tutorials/beginner/translation_transformer.html#data-sourcing-and-processing)\n- [The annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n- [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)","metadata":{"papermill":{"duration":0.009914,"end_time":"2022-09-17T19:53:09.399256","exception":false,"start_time":"2022-09-17T19:53:09.389342","status":"completed"},"pycharm":{"name":"#%% md\n"},"tags":[]}}]}